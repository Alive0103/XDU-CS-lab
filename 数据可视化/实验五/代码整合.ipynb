{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 道路数据处理\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 定义文件路径列表和对应的中文名称\n",
    "file_paths = [\n",
    "    './road2-12-9road/boundaryroad_with9road.geojson',\n",
    "    './road2-12-9road/crosswalkroad_with9road.geojson',\n",
    "    './road2-12-9road/laneroad_with9road.geojson',\n",
    "    './road2-12-9road/signalroad_with9road.geojson',\n",
    "    './road2-12-9road/stoplineroad_with9road.geojson'\n",
    "]\n",
    "\n",
    "file_names = [\n",
    "    '边界道路',\n",
    "    '人行横道路',\n",
    "    '车道线',\n",
    "    '信号灯道路',\n",
    "    '停车线道路'\n",
    "]\n",
    "\n",
    "# 创建子图，设定布局为 1 行 5 列\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))  # 可以调整 figsize 来改变图形的整体大小\n",
    "\n",
    "# 循环读取每个文件并绘制在对应的子图中\n",
    "for i, (file_path, ax) in enumerate(zip(file_paths, axes)):\n",
    "    # 读取 GeoJSON 文件\n",
    "    geo_df = gpd.read_file(file_path)\n",
    "    \n",
    "    # 绘制地图到子图，强制设置 aspect='equal'\n",
    "    geo_df.plot(ax=ax, aspect='equal')\n",
    "    \n",
    "    # 设置子图标题\n",
    "    ax.set_title(f'{file_names[i]}')\n",
    "\n",
    "# 调整布局以避免重叠\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json转excel\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义 type 值到中文名称的映射\n",
    "type_to_chinese = {\n",
    "    0: '未知',\n",
    "    1: '小型车辆',\n",
    "    2: '行人',\n",
    "    3: '非机动车',\n",
    "    4: '卡车',\n",
    "    5: '厢式货车_面包车',\n",
    "    6: '客车',\n",
    "    7: '静态物体',\n",
    "    8: '路牙',\n",
    "    9: '锥桶',\n",
    "    10: '手推车_三轮车',\n",
    "    11: '信号灯',\n",
    "    12: '门_阀门_闸机_出入口'\n",
    "}\n",
    "\n",
    "# 定义处理一块数据的函数\n",
    "def process_chunk(chunk):\n",
    "    chunk['position'] = chunk['position'].apply(json.loads)\n",
    "    chunk['x_coord'] = chunk['position'].apply(lambda pos: pos.get('x', None))\n",
    "    chunk['y_coord'] = chunk['position'].apply(lambda pos: pos.get('y', None))\n",
    "    \n",
    "    if 'velocity' in chunk.columns:\n",
    "        chunk['speed'] = chunk['velocity']\n",
    "    \n",
    "    if 'type' in chunk.columns:\n",
    "        chunk['vehicle_type'] = chunk['type']\n",
    "    \n",
    "    if 'orientation' in chunk.columns:\n",
    "        chunk['orientation_angle'] = chunk['orientation']\n",
    "    \n",
    "    if 'heading' in chunk.columns:\n",
    "        chunk['heading_angle'] = chunk['heading']\n",
    "    \n",
    "    if 'vehicle_type' in chunk.columns:\n",
    "        chunk = chunk[chunk['vehicle_type'].isin(type_to_chinese.keys())].copy()\n",
    "    \n",
    "    if 'speed' in chunk.columns:\n",
    "        def speed_status(speed):\n",
    "            if speed == 0:\n",
    "                return '静止'\n",
    "            elif speed < 5:\n",
    "                return '低速'\n",
    "            else:\n",
    "                return '高速'\n",
    "        \n",
    "        chunk.loc[:, 'speed_status'] = chunk['speed'].apply(speed_status)\n",
    "    \n",
    "    if 'orientation_angle' in chunk.columns:\n",
    "        def direction_category(orientation):\n",
    "            if -0.5 < orientation <= 0.5:\n",
    "                return '东'\n",
    "            elif 0.5 < orientation <= 1.5:\n",
    "                return '北'\n",
    "            elif -1.5 < orientation <= -0.5:\n",
    "                return '南'\n",
    "            else:\n",
    "                return '西'\n",
    "        \n",
    "        chunk.loc[:, 'direction'] = chunk['orientation_angle'].apply(direction_category)\n",
    "    \n",
    "    return chunk\n",
    "\n",
    "# 增量处理并按类型分块存储为Excel文件\n",
    "def incremental_process_by_type_to_excel(file_path, output_dir, chunk_size=50000):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        temp_chunk = []\n",
    "        \n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Processing {file_path}\")):\n",
    "            data = json.loads(line)\n",
    "            temp_chunk.append(data)\n",
    "\n",
    "            if len(temp_chunk) >= chunk_size:\n",
    "                df_chunk = pd.DataFrame(temp_chunk)\n",
    "                processed_chunk = process_chunk(df_chunk)\n",
    "\n",
    "                for vehicle_type, group_df in processed_chunk.groupby('vehicle_type'):\n",
    "                    chinese_name = type_to_chinese[vehicle_type]\n",
    "                    # 将每个数据块保存到单独的 Excel 文件\n",
    "                    output_file = os.path.join(output_dir, f'{chinese_name}_part_{i // chunk_size}.xlsx')\n",
    "                    \n",
    "                    # 保存到 Excel 文件\n",
    "                    group_df.to_excel(output_file, index=False)\n",
    "\n",
    "                temp_chunk = []  # 清空临时块\n",
    "\n",
    "        # 处理最后一块数据\n",
    "        if temp_chunk:\n",
    "            df_chunk = pd.DataFrame(temp_chunk)\n",
    "            processed_chunk = process_chunk(df_chunk)\n",
    "\n",
    "            for vehicle_type, group_df in processed_chunk.groupby('vehicle_type'):\n",
    "                chinese_name = type_to_chinese[vehicle_type]\n",
    "                output_file = os.path.join(output_dir, f'{chinese_name}_part_final.xlsx')\n",
    "                group_df.to_excel(output_file, index=False)\n",
    "\n",
    "# 批量处理多个 JSON 文件\n",
    "def process_multiple_json_files(file_list, output_dir, chunk_size=50000):\n",
    "    for file_path in file_list:\n",
    "        incremental_process_by_type_to_excel(file_path, output_dir, chunk_size)\n",
    "\n",
    "# 示例：批量处理多个 JSON 文件\n",
    "json_files = [\n",
    "    './part-00002-62742e18-8a7b-444f-91a8-41e5ebf9258f-c000.json',\n",
    "]\n",
    "output_dir = './数据处理'\n",
    "process_multiple_json_files(json_files, output_dir, chunk_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合表格\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# 定义源文件夹和目标文件夹路径\n",
    "source_folder = './数据处理/'\n",
    "target_folder = './处理结果/'\n",
    "\n",
    "# 如果目标文件夹不存在，创建它\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "# 遍历源文件夹中的所有Excel文件\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith('.xlsx'):  # 只处理Excel文件\n",
    "        file_path = os.path.join(source_folder, filename)\n",
    "        \n",
    "        # 读取每个Excel文件\n",
    "        data_df = pd.read_excel(file_path)\n",
    "        \n",
    "        # 提取所需的列\n",
    "        processed_data_df = data_df.loc[:, ['id', 'speed', 'x_coord', 'y_coord', 'type', 'time_meas', 'orientation_angle', 'heading_angle']]\n",
    "        \n",
    "        # 计算 'orientation_heading_diff' 列\n",
    "        processed_data_df.loc[:, 'orientation_heading_diff'] = processed_data_df['orientation_angle'] - processed_data_df['heading_angle']\n",
    "        \n",
    "        # 删除原来的 'orientation_angle' 和 'heading_angle' 列\n",
    "        processed_data_df = processed_data_df.drop(columns=['orientation_angle', 'heading_angle'])\n",
    "        \n",
    "        # 将 'time_meas' 列转换为日期时间格式，单位为微秒\n",
    "        processed_data_df['time_meas'] = pd.to_datetime(processed_data_df['time_meas'], unit='us')\n",
    "        \n",
    "        # 初始化时间分组\n",
    "        start_time = processed_data_df['time_meas'].min()\n",
    "        end_time = processed_data_df['time_meas'].max()\n",
    "        sheets_dict = {}\n",
    "        current_interval_start = start_time\n",
    "\n",
    "        # 按10分钟分组\n",
    "        while current_interval_start <= end_time:\n",
    "            current_interval_end = current_interval_start + timedelta(minutes=10)\n",
    "            interval_data = processed_data_df[(processed_data_df['time_meas'] >= current_interval_start) & (processed_data_df['time_meas'] < current_interval_end)]\n",
    "            \n",
    "            if not interval_data.empty:\n",
    "                sheet_name = f\"{current_interval_start.strftime('%Y%m%d_%H%M')} - {current_interval_end.strftime('%H%M')}\"\n",
    "                sheets_dict[sheet_name] = interval_data\n",
    "            \n",
    "            current_interval_start = current_interval_end\n",
    "\n",
    "        # 保存到新的Excel文件，每个10分钟区间为一个工作表\n",
    "        new_filename = 'new_' + filename\n",
    "        output_path = os.path.join(target_folder, new_filename)\n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            for sheet_name, data in sheets_dict.items():\n",
    "                writer_sheet_name = sheet_name[:31]  # 确保工作表名称不超过31字符\n",
    "                data.to_excel(writer, sheet_name=writer_sheet_name, index=False)\n",
    "\n",
    "        print(f\"处理完成: {new_filename}，保存在 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按时间划分合并\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义包含所有处理后Excel文件的文件夹路径\n",
    "source_folder = './处理结果/'\n",
    "output_folder = './合并结果/'\n",
    "\n",
    "# 如果输出文件夹不存在，创建它\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 用于存储所有数据的字典，按Sheet名称分组\n",
    "sheets_data = {}\n",
    "\n",
    "# 遍历处理后的Excel文件\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        file_path = os.path.join(source_folder, filename)\n",
    "        \n",
    "        # 读取文件中的所有Sheet\n",
    "        excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "        \n",
    "        # 遍历每个Sheet，将内容合并到sheets_data字典中\n",
    "        for sheet_name, data in excel_data.items():\n",
    "            if sheet_name not in sheets_data:\n",
    "                sheets_data[sheet_name] = []  # 创建新的列表用于存储该Sheet的所有数据\n",
    "            sheets_data[sheet_name].append(data)  # 将数据添加到相应的Sheet名称列表中\n",
    "\n",
    "# 将合并后的数据按Sheet名称保存到新的Excel文件中\n",
    "for sheet_name, data_list in sheets_data.items():\n",
    "    # 将所有数据合并为一个DataFrame\n",
    "    combined_data = pd.concat(data_list, ignore_index=True)\n",
    "    \n",
    "    # 保存每个合并后的Sheet为独立的Excel文件\n",
    "    output_path = os.path.join(output_folder, f\"{sheet_name}.xlsx\")\n",
    "    combined_data.to_excel(output_path, index=False)\n",
    "    \n",
    "    print(f\"合并完成并保存: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=7.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=7.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=6.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\envs\\shumo\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主选择页面生成完成\n"
     ]
    }
   ],
   "source": [
    "#绘图\n",
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyecharts.charts import Bar, Scatter, Line, Grid\n",
    "from pyecharts import options as opts\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # 将线程数限制为 1，避免内存泄漏\n",
    "\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 文件路径\n",
    "geojson_file_path = './车辆驾驶画像可视分析/road2-12-9road/boundaryroad_with9road.geojson'\n",
    "excel_files = ['./车辆驾驶画像可视分析/合并结果/20230413_0000 - 0010.xlsx', \n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0001 - 0011.xlsx',\n",
    "              './车辆驾驶画像可视分析/合并结果/20230413_0003 - 0013.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0004 - 0014.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0010 - 0020.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0011 - 0021.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0013 - 0023.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0014 - 0024.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0020 - 0030.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0021 - 0031.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0023 - 0033.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0024 - 0034.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0030 - 0040.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0031 - 0041.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0033 - 0043.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0034 - 0044.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0041 - 0051.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0040 - 0050.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0043 - 0053.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0044 - 0054.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0050 - 0100.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0051 - 0101.xlsx',\n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0053 - 0103.xlsx', \n",
    "               './车辆驾驶画像可视分析/合并结果/20230413_0054 - 0104.xlsx'\n",
    "              ]\n",
    "\n",
    "# 颜色配置\n",
    "colors = [\"blue\", \"orange\", \"green\", \"red\"]\n",
    "\n",
    "# 类型映射字典\n",
    "type_mapping = {\n",
    "    0: \"未识别\", 1: \"小型车辆\", 2: \"行人\",\n",
    "    3: \"非机动车\", 4: \"卡车\", 5: \"厢式货车\",\n",
    "    6: \"客车\", 7: \"静态物体\", 8: \"路牙\",\n",
    "    9: \"锥桶\", 10: \"手推车\", 11: \"信号灯\",\n",
    "    12: \"闸机\"\n",
    "}\n",
    "\n",
    "# 主HTML文件模板\n",
    "main_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Excel Analysis Selector</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; }}\n",
    "        .container {{ text-align: center; }}\n",
    "        .chart-container {{ margin: auto; width: 90%; }}\n",
    "        select {{ font-size: 16px; padding: 8px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"container\">\n",
    "    <h1>选择时间片</h1>\n",
    "    <select id=\"pageSelector\" onchange=\"selectPage()\">\n",
    "        <option value=\"\" disabled selected>选择一个待分析的时间段</option>\n",
    "        {options}\n",
    "    </select>\n",
    "    <div class=\"chart-container\" id=\"chartContainer\">\n",
    "        <!-- Placeholder for charts -->\n",
    "    </div>\n",
    "</div>\n",
    "<script>\n",
    "    function selectPage() {{\n",
    "        let selector = document.getElementById(\"pageSelector\");\n",
    "        let selectedValue = selector.value;\n",
    "        document.getElementById(\"chartContainer\").innerHTML = `<iframe src=\"${{selectedValue}}\" width=\"100%\" height=\"1000px\" frameborder=\"0\"></iframe>`;\n",
    "    }}\n",
    "</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML页面选项\n",
    "options_html = \"\"\n",
    "\n",
    "# 循环处理每个 Excel 文件\n",
    "for idx, excel_file_path in enumerate(excel_files):\n",
    "    data = pd.read_excel(excel_file_path, sheet_name='Sheet1')\n",
    "    file_name = os.path.basename(excel_file_path)\n",
    "\n",
    "    # 生成聚类图并转换为Base64编码\n",
    "    data_for_clustering = data[['speed', 'orientation_heading_diff']].dropna()\n",
    "    n_clusters = 4\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    clusters = kmeans.fit_predict(data_for_clustering)\n",
    "    data_for_clustering['cluster'] = clusters\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    for cluster_num in range(n_clusters):\n",
    "        cluster_data = data_for_clustering[data_for_clustering['cluster'] == cluster_num]\n",
    "        ax.scatter(cluster_data['speed'], cluster_data['orientation_heading_diff'], color=colors[cluster_num], label=f'簇 {cluster_num}', s=10)\n",
    "\n",
    "    # 绘制聚类中心\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], s=100, c='black', marker='x', label='中心点')\n",
    "    ax.set_xlabel('速度')\n",
    "    ax.set_ylabel('方向偏差')\n",
    "    ax.set_title('速度与方向偏差聚类分析')\n",
    "    ax.legend()\n",
    "\n",
    "    # 将图像转换为Base64编码\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format=\"png\")\n",
    "    plt.close(fig)\n",
    "    img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "    img_html = f'<div style=\"position:absolute; left:60%; bottom:15%; transform: translateX(-50%);\"><img src=\"data:image/png;base64,{img_base64}\" width=\"500\" height=\"330\"></div>'\n",
    "\n",
    "    # ======= 生成pyecharts的其他图表 =======\n",
    "    # 图表1：不同车种数量的条形统计图\n",
    "    type_counts = data['type'].value_counts()\n",
    "    bar_chart = (\n",
    "        Bar()\n",
    "        .add_xaxis([type_mapping.get(t, f\"未知类型 {t}\") for t in type_counts.index])\n",
    "        .add_yaxis(\"车种数量\", type_counts.values.tolist(), label_opts=opts.LabelOpts(is_show=False))\n",
    "        .set_global_opts(\n",
    "            xaxis_opts=opts.AxisOpts(name=\"车种类型\"),\n",
    "            yaxis_opts=opts.AxisOpts(name=\"数量\"),\n",
    "            legend_opts=opts.LegendOpts(pos_top=\"2%\", pos_left=\"center\", orient=\"horizontal\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 图表2：道路与坐标点图（散点图）\n",
    "    road_data = gpd.read_file(geojson_file_path)\n",
    "    scatter_chart = Scatter()\n",
    "    scatter_chart.set_global_opts(\n",
    "        xaxis_opts=opts.AxisOpts(type_=\"value\", name=\"X坐标\"),\n",
    "        yaxis_opts=opts.AxisOpts(type_=\"value\", name=\"Y坐标\")\n",
    "    )\n",
    "\n",
    "    for _, row in road_data.iterrows():\n",
    "        geometry = row.geometry\n",
    "        if geometry.geom_type == 'LineString':\n",
    "            coordinates = list(geometry.coords)\n",
    "        elif geometry.geom_type == 'Polygon':\n",
    "            coordinates = list(geometry.exterior.coords)\n",
    "        else:\n",
    "            continue\n",
    "        scatter_chart.add_xaxis([coord[0] for coord in coordinates])\n",
    "        scatter_chart.add_yaxis(\"道路\", [coord[1] for coord in coordinates], symbol_size=0.2, color=\"green\", label_opts=opts.LabelOpts(is_show=False))\n",
    "\n",
    "    for t, group in data.groupby('type'):\n",
    "        scatter_data = group[['x_coord', 'y_coord']].values.tolist()\n",
    "        color = colors[t % len(colors)]\n",
    "        scatter_chart.add_xaxis([coord[0] for coord in scatter_data])\n",
    "        scatter_chart.add_yaxis(type_mapping.get(t, f\"未知类型 {t}\"), [coord[1] for coord in scatter_data], symbol_size=1.5, color=color, label_opts=opts.LabelOpts(is_show=False))\n",
    "\n",
    "    # 图表3：速度统计图\n",
    "    line_chart = Line()\n",
    "    line_chart.add_xaxis(data.index.tolist())\n",
    "    for t, group in data.groupby('type'):\n",
    "        color = colors[t % len(colors)]\n",
    "        line_chart.add_yaxis(type_mapping.get(t, f\"未知类型 {t}\"), group['speed'].tolist(), is_smooth=True, color=color, symbol=\"circle\", symbol_size=3)\n",
    "\n",
    "    line_chart.set_global_opts(\n",
    "        xaxis_opts=opts.AxisOpts(name=\"ID\", axislabel_opts=opts.LabelOpts(is_show=False)),\n",
    "        yaxis_opts=opts.AxisOpts(name=\"速度\"), \n",
    "        legend_opts=opts.LegendOpts(is_show=False)\n",
    "    )\n",
    "\n",
    "    # 使用 Grid 布局，将所有图表添加到同一页面\n",
    "    grid = Grid(init_opts=opts.InitOpts(width=\"1200px\", height=\"1000px\"))\n",
    "    grid.add(bar_chart, grid_opts=opts.GridOpts(pos_left=\"5%\", pos_right=\"60%\", pos_top=\"12%\", pos_bottom=\"55%\"))\n",
    "    grid.add(scatter_chart, grid_opts=opts.GridOpts(pos_left=\"60%\", pos_right=\"5%\", pos_top=\"12%\", pos_bottom=\"55%\"))\n",
    "    grid.add(line_chart, grid_opts=opts.GridOpts(pos_left=\"5%\", pos_right=\"60%\", pos_top=\"55%\", pos_bottom=\"5%\"))\n",
    "\n",
    "    # 输出单独的 HTML 文件\n",
    "    single_output_path = f\"./result/分表_{idx + 1}.html\"\n",
    "    grid.render(single_output_path)\n",
    "\n",
    "    # 将页面选项添加到主HTML文件的下拉菜单\n",
    "    options_html += f'<option value=\"{single_output_path}\">{file_name}</option>'\n",
    "\n",
    "    # 在每个HTML文件的底部插入聚类图HTML\n",
    "    with open(single_output_path, \"r+\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "        html_content = html_content.replace(\"</body>\", f\"{img_html}</body>\")\n",
    "        file.seek(0)\n",
    "        file.write(html_content)\n",
    "        file.truncate()\n",
    "\n",
    "# 创建主HTML页面\n",
    "main_html_content = main_html.format(options=options_html)\n",
    "with open(\"主界面.html\", \"w\", encoding=\"utf-8\") as main_file:\n",
    "    main_file.write(main_html_content)\n",
    "\n",
    "print(\"主选择页面生成完成\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shumo",
   "language": "python",
   "name": "shumo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "nbTranslate": {
   "displayLangs": [
    "cn",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "cn",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
